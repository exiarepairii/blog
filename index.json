[{"content":"","permalink":"http://qiaosu.gitee.io/post/second/","summary":"","title":"Second"},{"content":"主成分分析意在降维，因子分析意在分析数据内在结构。二者目的不同，但采取了很多相同的步骤，因此如果只用结论就非常容易混淆。为了明确二者差异，首先给出步骤以及推导。\n步骤 假设有$m$条样本数据，每条样本有$n$维特征，得到样本矩阵\n$$ X^* = \\left( \\begin{matrix} x^*_{11} \u0026amp; x^*_{12} \u0026amp; \\dots \u0026amp; x^*_{1n} \\\\\nx^*_{21} \u0026amp; x^*_{22} \u0026amp; \\dots \u0026amp; x^*_{2n} \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nx^*_{m1} \u0026amp; x^*_{m2} \u0026amp; \\dots \u0026amp; x^*_{mn} \\\\\n\\end{matrix} \\right) = \\left( \\begin{matrix} x^*_{1} \u0026amp; x^*_{2} \u0026amp; \\dots \u0026amp; x^*_{n} \\\\ \\end{matrix} \\right) $$ 为了方便计算协方差，首先对样本进行中心化处理（用协方差\n","permalink":"http://qiaosu.gitee.io/post/first/","summary":"主成分分析意在降维，因子分析意在分析数据内在结构。二者目的不同，但采取了很多相同的步骤，因此如果只用结论就非常容易混淆。为了明确二者差异，首先给出步骤以及推导。\n步骤 假设有$m$条样本数据，每条样本有$n$维特征，得到样本矩阵\n$$ X^* = \\left( \\begin{matrix} x^*_{11} \u0026amp; x^*_{12} \u0026amp; \\dots \u0026amp; x^*_{1n} \\\\\nx^*_{21} \u0026amp; x^*_{22} \u0026amp; \\dots \u0026amp; x^*_{2n} \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nx^*_{m1} \u0026amp; x^*_{m2} \u0026amp; \\dots \u0026amp; x^*_{mn} \\\\\n\\end{matrix} \\right) = \\left( \\begin{matrix} x^*_{1} \u0026amp; x^*_{2} \u0026amp; \\dots \u0026amp; x^*_{n} \\\\ \\end{matrix} \\right) $$ 为了方便计算协方差，首先对样本进行中心化处理（用协方差","title":"First"},{"content":"主成分分析意在降维，因子分析意在分析数据内在结构。二者目的不同，但采取了很多相同的步骤，因此如果只用结论就非常容易混淆。为了明确二者差异，首先给出步骤以及推导。\n步骤 假设有$m$条样本数据，每条样本有$n$维特征，得到样本矩阵\n$$ X^* = \\left( \\begin{matrix} x^*_{11} \u0026amp; x^*_{12} \u0026amp; \\dots \u0026amp; x^*_{1n} \\\\\nx^*_{21} \u0026amp; x^*_{22} \u0026amp; \\dots \u0026amp; x^*_{2n} \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nx^*_{m1} \u0026amp; x^*_{m2} \u0026amp; \\dots \u0026amp; x^*_{mn} \\\\\n\\end{matrix} \\right) = \\left( \\begin{matrix} x^*_{1} \u0026amp; x^*_{2} \u0026amp; \\dots \u0026amp; x^*_{n} \\\\ \\end{matrix} \\right) $$ 为了方便计算协方差，首先对样本进行中心化处理（用协方差做主成分分析也可以不中心化，证明差不多），得到$X$，有$E(x_i) = 0$。\n计算$C = \\frac{1}{m}X^TX$，则$C$是$X$的协方差矩阵，证明如下：\n$$ x_1^Tx_1 = \\sum_{i=1}^{m}x^2_{i1} = m\\left(E(x_1^2) - E^2(x_1)\\right)=mCov(x_1,x_1) $$\n$$ x_1^Tx_2 = \\sum_{i=1}^{m}x_{i1}x_{i2} = m\\left(E(x_1x_2) - E(x_1)E(x_2)\\right)=mCov(x_1,x_2) $$\n$$ X^TX = m\\left( \\begin{matrix} Cov(x_1,x_1) \u0026amp; Cov(x_1,x_2) \u0026amp; \\dots \u0026amp; Cov(x_1,x_n) \\\\\nCov(x_2,x_1) \u0026amp; Cov(x_2,x_2) \u0026amp; \\dots \u0026amp; Cov(x_2,x_n) \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nCov(x_n,x_1) \u0026amp; Cov(x_n,x_2) \u0026amp; \\dots \u0026amp; Cov(x_n,x_n) \\\\\n\\end{matrix} \\right) $$\n故$C$即为$X$的协方差矩阵，得证。\n根据协方差矩阵的性质，知$C$为实对称矩阵。实对称矩阵必然可以相似对角化，故存在正交矩阵$P$，使得\n$$ P^TCP = \\Lambda = \\left( \\begin{matrix} \\lambda_1 \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \\\\\n0 \u0026amp; \\lambda_2 \u0026amp; \\dots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\n0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; \\lambda_n \\\\\n\\end{matrix} \\right) $$\n其中$P$由$C$经正交化后的特征向量组成，记为\n$$ P = \\left( \\begin{matrix} p_1 \u0026amp; p_2 \u0026amp; \\dots \u0026amp; p_n \\\\\n\\end{matrix} \\right) $$\n主成分分析和因子分析从这一步开始不同。\n主成分分析 主成分分析的目标是把样本维度从$n$降低到$k$（$k = 1,2,\\dots,n-1$）。\n至此，我们其实已经在不损失信息的前提下，将原样本$X$中盘根错节的特征相互分离，得到了新样本$XP$。接下来只需要保留$XP$中包含信息最多的$k$个特征，即可达到降维的目的。\n为什么要将$XP$作为新样本呢？\n$$ \\Lambda = P^TCP = P^T(\\frac{1}{m}X^TX)P = \\frac{1}{m}(XP)^TXP\\\\\n$$\n对比$X$的协方差矩阵$C = \\frac{1}{m}X^TX$，可以发现上式意味着$\\Lambda$是$XP$的协方差矩阵。\n如果将$XP$看作新样本，我们会发现$XP$的各个特征互不相关，说明特征之间不包含重复的信息。这是非常优秀的性质，如果以这样的数据作为样本，在建模时我们就不必考虑特征的相互影响。\n而另一方面，线性代数中向量的坐标是由向量空间的基确定的。\n$$ XE = XPP^T $$\n根据基的概念，可以把$X$看作样本在基$E$下的坐标，而$XP$则是样本在基$P^T$下的坐标。\n因此，将$XP$作为新样本不仅有利于剔除不重要的成分，还保留了原始样本中的所有信息。\n那要如何选择主成分呢？显然我们希望得到的主成分包含更多原始样本的信息（即经过PCA损耗的信息更少），因此应该根据主成分包含的信息量进行筛选。而方差反映了特征的变动幅度，变动大的特征自然包含的信息就更多，故保留$XP$中方差最大的$k$个特征即可得到$m$个样本关于$k$个主成分的取值。\n因子分析 假设存在数个不可观测的潜在变量，由这些潜在变量的变化影响了观测样本中各个特征的变化。因子分析希望将这些潜在变量从样本数据中分离，并找出样本特征与这些潜在变量之间的关系。用数学公式表示如下\n$$ X = FA + \\varepsilon $$\n其中$A$被称作因子载荷矩阵，$F$为公共因子，$\\varepsilon$为特殊因子带来的影响，类似于回归分析中的随机误差。\n不考虑特殊因子，式中仍有两个未知项，难以分解。假设$Cov(F) = E,Cov(F,\\varepsilon)=0, \\varepsilon_i \\sim N(0,\\sigma_i^2)$，则有\n$$ Cov(X) = AA^T + Cov(\\varepsilon) = AA^T + D $$\n经此运算，式中的未知项$F$即被消去。根据前文的证明，有\n$$ \\begin{align} Cov(X) \u0026amp;= C = P\\Lambda P^T \\\\\n\u0026amp;= \\sum_{i=1}^n\\lambda_ip_ip_i^T \\\\\n\u0026amp;= \\sum_{i=1}^k\\lambda_ip_ip_i^T + D \\\\\n\u0026amp;= AA^T + D \\end{align} $$\n其中\n$$ A = \\left( \\begin{matrix} \\sqrt{\\lambda_1}p_1 \u0026amp; \\sqrt{\\lambda_2}p_2 \u0026amp; \\dots \\sqrt{\\lambda_k}p_k \\end{matrix} \\right) $$\n此式即为因子载荷矩阵的解。得到因子载荷矩阵后，可以将因子载荷看作观测值，因子得分看作系数，通过最小二乘法求得因子得分$F$。\n因子分析的目的在于解释样本变量的内在结构。为了让因子更方便解释，可以利用类似前文PCA的原理，通过正交变换，尽可能扩大$F$各因子方差，即\n$$ X = FA + \\varepsilon = FRR^TA + \\varepsilon $$\n这就是因子旋转，具体的旋转方法有很多，由于和本文关系不大，不再赘述。\nSPSS操作 SPSS进行主成分分析和因子分析非常容易混淆。\n使用SPSS进行主成分分析时给出的成分矩阵实际上是因子载荷矩阵$A$，故应该对其中列向量除以相应的$\\sqrt{\\lambda}$才能得到主成分系数$P$。这就使得SPSS给出的主成分分析和因子分析（不加旋转的）的前半部分结果是完全相同的，让本就步骤相似的两个方法更加难以区分。\n比较异同 二者的目的不同。主成分分析的目标是降维，通过坐标变换，在尽可能多地保留数据信息的前提下，将变量线性组合以降低维度。因子分析的目标是分析，在假定存在无法观测的潜在因子的基础上，分解出因子载荷与主要因子，用来分析数据的内在结构。只看分析结果的话，主成分是对原有特征的聚合，因子是对原有特征的分解。\n二者有很多步骤相同。两种分析都对样本进行了中心化处理，都计算了协方差矩阵，都进行了相似对角化，一直到筛选特征这一步为止，二者所做的工作都是相同的。\n我认为步骤相同只是巧合。主成分分析进行这些步骤是为了找到正交矩阵$P$，从而将特征相互剥离，以保证在剔除特征时尽可能多的保留信息，且不影响其他未被剔除的特征。因子分析进行这些步骤则是为了消除$F$，从而求解因子载荷矩阵。\n","permalink":"http://qiaosu.gitee.io/post/%E6%B5%85%E6%9E%90%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E5%92%8C%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/","summary":"主成分分析意在降维，因子分析意在分析数据内在结构。二者目的不同，但采取了很多相同的步骤，因此如果只用结论就非常容易混淆。为了明确二者差异，首先给出步骤以及推导。\n步骤 假设有$m$条样本数据，每条样本有$n$维特征，得到样本矩阵\n$$ X^* = \\left( \\begin{matrix} x^*_{11} \u0026amp; x^*_{12} \u0026amp; \\dots \u0026amp; x^*_{1n} \\\\\nx^*_{21} \u0026amp; x^*_{22} \u0026amp; \\dots \u0026amp; x^*_{2n} \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nx^*_{m1} \u0026amp; x^*_{m2} \u0026amp; \\dots \u0026amp; x^*_{mn} \\\\\n\\end{matrix} \\right) = \\left( \\begin{matrix} x^*_{1} \u0026amp; x^*_{2} \u0026amp; \\dots \u0026amp; x^*_{n} \\\\ \\end{matrix} \\right) $$ 为了方便计算协方差，首先对样本进行中心化处理（用协方差做主成分分析也可以不中心化，证明差不多），得到$X$，有$E(x_i) = 0$。\n计算$C = \\frac{1}{m}X^TX$，则$C$是$X$的协方差矩阵，证明如下：\n$$ x_1^Tx_1 = \\sum_{i=1}^{m}x^2_{i1} = m\\left(E(x_1^2) - E^2(x_1)\\right)=mCov(x_1,x_1) $$","title":"浅析主成分分析和因子分析"}]