[{"content":"Denoising Diffusion Probabilistic Models https://arxiv.org/abs/2006.11239\n\n第一篇用diffusion model做图像生成的\n Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples.\n 背景 逆向过程 由标准高斯噪声$x_T$转移到原始输入$x_0$的马尔可夫链。定义\n此处$p_{\\theta}(x_{0:T})=p_{\\theta}(x_0,x_1,\\dots,x_T)$，对$x_{1:T}$积分可得原始输入$x_0$的边缘分布\n$$ p_{\\theta}(x_0) = \\int p_{\\theta}(x_{0:T})dx_{1:T} $$\n每一步的转移概率服从高斯分布，其均值和方差由前一个状态计算得到。\n前向（扩散）过程 由原始输入$x_0$逐渐加噪声，得到各向独立的高斯噪声$x_T\\sim N(x_T;0,I)$，也是一个马尔可夫过程\n转移概率由超参$\\beta_t$和前一个状态$x_{t-1}$共同决定。看起来$\\beta_t \\in [0,1]$，而且方差越大均值约小，这个有什么深意？\n前向过程中任意时刻的条件概率都可以写作\n$$ q(x_t|x_0) = N(x_t;\\sqrt{\\overline{\\alpha}_t}x_0,(1-\\overline{\\alpha}_t)I); \\quad \\text{where} \\quad\\alpha_t := 1-\\beta_t ,\\quad \\overline{\\alpha}t := \\prod{s=1}^t \\alpha_s $$\n训练目标 最小化NLL的上界\n由詹森不等式https://en.wikipedia.org/wiki/Jensen\u0026rsquo;s_inequality\n$$ \\begin{aligned} -\\log p_{\\theta}(x_0) \u0026amp;=-\\log\\int q(x_{1:T}|x_0)\\frac{p_{\\theta}(x_{0:T}|x_{1:T})p_{\\theta}(x_{1:T})}{q(x_{1:T}|x_0)}dx_{1:T} \\ \u0026amp;= -\\log E_q(\\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}|x_0)})\\ \u0026amp;\\le E_q(-\\log \\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}|x_0)}) \\ \u0026amp;=E_q(-\\log \\frac{p(x_T)\\prod_{t=1}^{T}p_{\\theta}(x_{t-1}|x_{t})}{\\prod_{t=1}^{T}q(x_t|x_{t-1})}) \\ \u0026amp;= E_q(-\\log p(x_T) - \\sum_{t\\ge 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1})})=:L \\ \u0026amp;=E_q(-\\log p(x_T) - \\sum_{t\u0026gt; 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1})} - \\log \\frac{p_{\\theta}(x_{0}|x_1)}{q(x_1|x_{0})}) \\ \\end{aligned} $$\n又\n$$ \\begin{aligned} q(x_t|x_{t-1}) \u0026amp;= q(x_t|x_{t-1},x_0) \\\u0026amp;=\\frac{q(x_{t-1}|x_t,x_0)q(x_t,x_0)}{q(x_{t-1},x_0)} \\ \u0026amp;= q(x_{t-1}|x_t,x_0) \\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}\\end{aligned} $$\n因此\n$$ \\begin{aligned} L \u0026amp;=E_q(-\\log p(x_T) - \\sum_{t\u0026gt; 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1},x_0)}\\cdot \\frac{q(x_{t-1}|x_0)}{q(x_{t}|x_0)} - \\log \\frac{p_{\\theta}(x_{0}|x_1)}{q(x_1|x_{0})}) \\end{aligned} $$\n其中\n$$ \\begin{aligned} \\sum_{t\u0026gt; 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1},x_0)}\\cdot \\frac{q(x_{t-1}|x_0)}{q(x_{t}|x_0)} \u0026amp;= \\sum_{t\u0026gt; 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1},x_0)} + \\sum_{t\u0026gt; 1} \\log \\frac{q(x_{t-1}|x_0)}{q(x_{t}|x_0)} \\\u0026amp;= \\sum_{t\u0026gt; 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1},x_0)} + \\log \\frac{q(x_{1}|x_0)}{q(x_{T}|x_0)}\\end{aligned} $$\n得到\n$$ \\begin{aligned} L \u0026amp;=E_q(-\\log p(x_T) - \\sum_{t\u0026gt; 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1},x_0)} - \\log \\frac{q(x_{1}|x_0)}{q(x_{T}|x_0)} - \\log \\frac{p_{\\theta}(x_{0}|x_1)}{q(x_1|x_{0})}) \\\u0026amp;= E_q(-\\log \\frac{p(x_T)}{q(x_T|x_0)} - \\sum_{t\u0026gt; 1} \\log \\frac{p_{\\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1},x_0)} - \\log p_{\\theta}(x_{0}|x_1))\\end{aligned} $$\n用KL散度重写成\n训练内容 前向过程（2）式中的超参$\\beta_t$、模型架构、反向过程的高斯分布参数（均值、方差）\n前向过程 把$\\beta_t$作为固定的超参数，相当于前向过程分布没有需要学习的参数了。\nloss里的$L_T$就是常数了（$p(x_T)$是标准正态）\n逆向过程 设$p_{\\theta}(x_{t-1}|x_t)$的方差$\\Sigma_{\\theta}(x_t,t)=\\sigma_t^2I$，即一个时间相关的无需训练的常量。\n由高斯分布KL散度推导，得到\n$$ L_{t-1} = E_q(\\frac{1}{2\\sigma_t^2}||\\widetilde{\\mu_t}(x_t,x_0)- \\mu_{\\theta}(x_t,t)||^2) + C $$\n所以最直接的建模就是找到一组参数$\\theta$使得$\\mu_{\\theta}(x_t,t)$接近$\\widetilde{\\mu_t}(x_t,x_0)$。\n因为前向过程是从$x_0$逐渐扩散成$x_T$，其中每一步的转移概率分布都取决于前一次的转移概率分布，相当于是每一步的转移概率分布都和初始分布有关。可以把前向过程里的$x_t$扩写成$x_t(x_0,\\epsilon) = \\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\epsilon$，其中$\\epsilon \\sim N(0,I)$，代入上式\nEq(10)推导见（https://blog.csdn.net/qq_40714949/article/details/126643111）\n由Eq(9)，可以选择参数化$\\mu_{\\theta}(x_t,t)$如下\n至此，$L_{t-1}$只剩下噪声$\\epsilon$未知，建模$\\epsilon_{\\theta}$，根据输入$x_t$和加噪步数$t$预测噪声$\\epsilon$。\n推理：\n 要采样$x_{t-1} \\sim p_{\\theta}(x_{t-1}|x_t)$，根据Eq(1)，就是要确定正态分布的均值$\\mu_{\\theta}(x_t,t)$和方差$\\Sigma_{\\theta}(x_t,t)$ 本节开头假设方差为某固定常数，只需要依Eq(11)计算均值就可以得到分布估计。 求的分布后，随机采样就得到了去噪图像$x_{t-1}$ t-=1，回到step1  ","permalink":"https://exiarepairii.github.io/blog/post/denoising-diffusion-model%E7%AC%94%E8%AE%B0/","summary":"Denoising Diffusion Probabilistic Models https://arxiv.org/abs/2006.11239\n\n第一篇用diffusion model做图像生成的\n Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples.\n 背景 逆向过程 由标准高斯噪声$x_T$转移到原始输入$x_0$的马尔可夫链。定义\n此处$p_{\\theta}(x_{0:T})=p_{\\theta}(x_0,x_1,\\dots,x_T)$，对$x_{1:T}$积分可得原始输入$x_0$的边缘分布\n$$ p_{\\theta}(x_0) = \\int p_{\\theta}(x_{0:T})dx_{1:T} $$\n每一步的转移概率服从高斯分布，其均值和方差由前一个状态计算得到。\n前向（扩散）过程 由原始输入$x_0$逐渐加噪声，得到各向独立的高斯噪声$x_T\\sim N(x_T;0,I)$，也是一个马尔可夫过程\n转移概率由超参$\\beta_t$和前一个状态$x_{t-1}$共同决定。看起来$\\beta_t \\in [0,1]$，而且方差越大均值约小，这个有什么深意？\n前向过程中任意时刻的条件概率都可以写作\n$$ q(x_t|x_0) = N(x_t;\\sqrt{\\overline{\\alpha}_t}x_0,(1-\\overline{\\alpha}_t)I); \\quad \\text{where} \\quad\\alpha_t := 1-\\beta_t ,\\quad \\overline{\\alpha}t := \\prod{s=1}^t \\alpha_s $$","title":"Denoising Diffusion Probabilistic Models阅读笔记"},{"content":"条件随机场是一个序列模型，被广泛应用在NLP的标注任务上。\n目标 假设输入一个序列 $$ X = \\left(x_1,x_2,\\dots,x_n\\right) $$ CRF模型需要预测序列中每个元素$x_i$对应的标签$y_i$，即输出一个序列 $$ \\hat{Y} = \\left(\\hat{y}_1,\\hat{y}_2,\\dots,\\hat{y}_n\\right) $$\n假设  只有相邻的标签之间具有依赖关系（线性链、无向图）。 $y_i$依赖于X  训练 前向传播 特征工程 转移特征 构造$K_1$条关于$y_i$和$y_{i-1}$​的特征，即 $$ t_k = t_k(y_{i-1},y_{i},x,i) = \\begin{cases} 1, \\text{condition_1} \\\\\n0, \\text{condition_0} \\end{cases} $$\n状态特征 构造$K_2$条关于$y_i$的特征，即\n$$ s_k = s_k(y_{i},x,i) = \\begin{cases} 1, \\text{condition_1} \\\\\n0, \\text{condition_0} \\end{cases} $$\n计算概率 假设每个$y_i$都有两种可能的取值，即$y_i \\in {1,2}$。当序列长度等于$3$时，所有可能的路径如下图所示\n如果$Y = (1,2,1)$，即为下图中的红色路径\n我们对如图所示的红色路径进行上文所述的特征工程。\n转移特征共有$K_1\\times 2$个（$i \\in {2,3}$），状态特征共有$K_2 \\times 3$个（序列包含$3$个结点）。将这些特征拼接起来，我们将得到一个长度为$K = 2K_1+3K_2$的一维向量，这个向量完全是由$0$和$1$组成的。\n将这个向量馈入一个单层的感知机（没有激活函数），我们将得到一个数值，可以视为该路径的得分。\n本例中每个$y_i$有$2$种可能的取值，所以总共存在$2^3$条可能的路径。对这些路径全部进行特征工程，我们将得到$2^3=8$个由0和1组成的向量，可以表示为一个$8$行$K$列的矩阵$X_F$。\n直接把矩阵$X_F$馈入感知机，我们就可以得到一个$8$行$1$列的向量$Y^*$，其中的每个元素代表了对应路径的得分。\n对该向量使用softmax归一化，即可将每个元素转换为给定$X$条件下出现对应路径的概率。\nLoss 统计学习方法里说是通过极大化训练数据的对数似然函数来求模型参数，即\n$$ W = \\mathop{\\arg\\max}\\limits_{W} \\log \\prod_{X,Y} P_W(Y|X)^{\\widetilde{P}(X|Y)} = \\mathop{\\arg\\max}\\limits_{W} \\log \\sum_{X,Y} \\widetilde{P}(X|Y) \\log P_W(Y|X) $$\n如果使用梯度下降法求解，那么相应的就是最小化负对数似然函数（negative log-likelihood），即pytorch中的NLLLoss。\n预测 使用CRF预测即找出在给定$X$的条件下出现概率最大的路径。\n理论上可以通过遍历所有路径，并根据前文前向传播的内容算出对于的概率，从中找出概率最大的路径即可。\n当路径空间很大时，遍历是不可行的。一般使用维特比算法求解。\n步骤：\n 遍历$Y_2$所有可能状态，对每一个状态找使$(Y_0,Y_1,Y_2)$转移概率最大的$Y_1$，记下来并形成路径$S_1$。 遍历$Y_i$所有可能状态，对每一个状态找使$(S_{i-1},Y_{i})$转移概率最大的$S_{i-1}$，记下来并形成路径$S_i$。 对终点$Y_{n+1}$，找使$(S_n,Y_{n+1})$转移概率最大的$S_{n}$，记下来。  参考：https://www.zhihu.com/question/20136144\n需要注意的是，第$i$步转移的概率是$P(Y_0=\\text{START},Y_1 = y_1,\\dots,Y_i = y_i,Y_{n+1}=\\text{STOP}|X)$，既需要考虑走过前面以确定路径的概率，还要考虑能够在第$n$步结束的概率。\n","permalink":"https://exiarepairii.github.io/blog/post/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%E7%AC%94%E8%AE%B0/","summary":"条件随机场是一个序列模型，被广泛应用在NLP的标注任务上。\n目标 假设输入一个序列 $$ X = \\left(x_1,x_2,\\dots,x_n\\right) $$ CRF模型需要预测序列中每个元素$x_i$对应的标签$y_i$，即输出一个序列 $$ \\hat{Y} = \\left(\\hat{y}_1,\\hat{y}_2,\\dots,\\hat{y}_n\\right) $$\n假设  只有相邻的标签之间具有依赖关系（线性链、无向图）。 $y_i$依赖于X  训练 前向传播 特征工程 转移特征 构造$K_1$条关于$y_i$和$y_{i-1}$​的特征，即 $$ t_k = t_k(y_{i-1},y_{i},x,i) = \\begin{cases} 1, \\text{condition_1} \\\\\n0, \\text{condition_0} \\end{cases} $$\n状态特征 构造$K_2$条关于$y_i$的特征，即\n$$ s_k = s_k(y_{i},x,i) = \\begin{cases} 1, \\text{condition_1} \\\\\n0, \\text{condition_0} \\end{cases} $$\n计算概率 假设每个$y_i$都有两种可能的取值，即$y_i \\in {1,2}$。当序列长度等于$3$时，所有可能的路径如下图所示\n如果$Y = (1,2,1)$，即为下图中的红色路径\n我们对如图所示的红色路径进行上文所述的特征工程。\n转移特征共有$K_1\\times 2$个（$i \\in {2,3}$），状态特征共有$K_2 \\times 3$个（序列包含$3$个结点）。将这些特征拼接起来，我们将得到一个长度为$K = 2K_1+3K_2$的一维向量，这个向量完全是由$0$和$1$组成的。\n将这个向量馈入一个单层的感知机（没有激活函数），我们将得到一个数值，可以视为该路径的得分。\n本例中每个$y_i$有$2$种可能的取值，所以总共存在$2^3$条可能的路径。对这些路径全部进行特征工程，我们将得到$2^3=8$个由0和1组成的向量，可以表示为一个$8$行$K$列的矩阵$X_F$。","title":"条件随机场笔记"},{"content":"Attention发展历程: https://www.cnblogs.com/robert-dlut/p/5952032.html\nAttention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf\nillustrated-transformer: http://jalammar.github.io/illustrated-transformer/\nTransformer Transformer具有一个Encoder-Decoder的结构，文章中说这也是目前（2017年）神经序列转导模型的主流结构。\nPositional Encoding 对每个token的embedding向量，维度为$1\\times d_\\text{model}$，将该token在句中的位置记作$pos$，则对向量中的第$i$个元素，其Positional Encoding记为$PE$，公式如下： $$ PE(pos,2i) = sin(pos/10000^{2i/d_\\text{model}})\\\\\nPE(pos,2i+1) = cos(pos/10000^{2i/d_\\text{model}}) $$ 将$PE$和embedding向量相加，就完成了Positional Encoding。\nEncoder Encoder包含6个相同的编码层\nEncoder Layer 假设$n$个token经过embedding，并加入了位置信息（Positional Encoding）之后，记作矩阵$X$，维度为$n\\times d_\\text{X}$。$X$首先进入Multi-Head Attention层。\nMulti-Head Attention 对普通的Self-Attention，首先初始化三个可训练的、大小为$d_\\text{X} \\times d_{\\text{model}}$的权重矩阵$W^Q,W^K,W^V$。由矩阵乘积$X \\times W$分别得到$Q,K,V$三个矩阵，大小为$n\\times d_\\text{model}$。从而根据如下公式计算输出： $$ \\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_\\text{model}}})V $$ 最终输出尺寸为$ n\\times d_\\text{model}$。若不使用矩阵形式表示，普通Self-Attention相当于对每个token的$q$，计算与其他token的$k$向量的点积，softmax归一化之后作为该token的权重，最后将所有token的向量加权平均作为输出。\nMulti-Head Attention相当于使用数个不同的权重矩阵$W_i$，并行计算Self-Attention，最后将计算结果横向拼接。\n假设共有$8$个这样并行的Self-Attention层，若每一层Self-Attention的权重矩阵大小为$d_\\text{X} \\times d_\\text{v}$，则输出尺寸为$n\\times 8d_v$。论文中取$d_v=d_{model}/8$，从而在学习更多信息的前提下，使运算量与普通Self-Attention类似。\n对拼接后的结果$x_c$，乘上额外的参数矩阵$W^O$，最终得到输出尺寸为$ n\\times d_\\text{model}$，其中$W^O$尺寸为$8d_v\\times d_\\text{model}$。\n得到了该层的输出后，和残差网络类似的，论文使用了残差连接（residual connection）的方法连接两层，即将原始输入和该层输出直接相加，之后再对此应用Layer Normalization，传入下一层。公式表达如下： $$ \\text{LayerNorm}(x+\\text{Sublayer}(x)) $$\nPosition-wise Feed-Forward Networks Multi-Head Attention层后紧跟两个全连接层。其中第一个全连接层的神经元数量$d_{ff}=2048$，激活函数为ReLU，第二层的神经元数量为$d_\\text{model}$。公式表达如下： $$ \\text{FFN} = \\max(0,xW_1+b_1)W_2+b_2 $$ 对不同位置的token，全连接层的参数是共享的。 论文说这类似于两个卷积核尺寸为$1\\times1$​ 的卷积层，这样的卷积层和全连接层可以相互转换，并且根据卷积核维度随意调节输出维度。要注意的是，该前馈网络的对象是每个token的向量，而非整句话。\n最后对结果应用残差链接和层标准化，即： $$ \\text{LayerNorm}(x+\\text{Sublayer}(x)) $$\nDecoder Decoder同样由6个相同的Decoder Layer组成。和Encoder不同的是，每个Decoder Layer都在第一层Multi-Head Attention之前加入了一个将之前输出结果作为输入的Masked Multi-Head Attention。该层输出结果和原始输入（总模型输出结果）进行Add\u0026amp;Norm之后，和Encoder的输出一同输入Multi-Head Attention层。\nMasked Multi-Head Attention  需要注意，Transformer 仍属于 Seq2Seq 模型，尽管 Encoder 部分可以并行的生成输入表示，Decoder 部分仍需要以自回归的方式逐词生成输出序列。自然的，在按照该方式生成输出序列时，Decoder 只能从当前词前面的词中获取信息（使后面序列的 Attention 值为0），因此需要加入 Mask（掩码）。\n具体方式是在相关性系数矩阵上累加一个 Mask 矩阵。该矩阵在需要 Mask 的位置的值为 -inf（具体实现时是一个非常小的数，比如 1e-9），其余位置为 0，这样在进行了 Softmax 归一化操作之后，被掩码掉的位置计算得到的权重便近似为 0。\n 修改后的矩阵化公式为： $$ \\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_\\text{model}}}+Mask)V $$\nLinear+Softmax 经过了Encoder和Decoder的运算，最后的结果将由Linear层转换为维度为$1\\times d_\\text{vocab}$的向量，然后经过Softmax函数将其转换为概率，从而找到概率最高的单词作为下一个输出（argmax）\n","permalink":"https://exiarepairii.github.io/blog/post/attention_is_all_you_need%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","summary":"Attention发展历程: https://www.cnblogs.com/robert-dlut/p/5952032.html\nAttention Is All You Need: https://arxiv.org/pdf/1706.03762.pdf\nillustrated-transformer: http://jalammar.github.io/illustrated-transformer/\nTransformer Transformer具有一个Encoder-Decoder的结构，文章中说这也是目前（2017年）神经序列转导模型的主流结构。\nPositional Encoding 对每个token的embedding向量，维度为$1\\times d_\\text{model}$，将该token在句中的位置记作$pos$，则对向量中的第$i$个元素，其Positional Encoding记为$PE$，公式如下： $$ PE(pos,2i) = sin(pos/10000^{2i/d_\\text{model}})\\\\\nPE(pos,2i+1) = cos(pos/10000^{2i/d_\\text{model}}) $$ 将$PE$和embedding向量相加，就完成了Positional Encoding。\nEncoder Encoder包含6个相同的编码层\nEncoder Layer 假设$n$个token经过embedding，并加入了位置信息（Positional Encoding）之后，记作矩阵$X$，维度为$n\\times d_\\text{X}$。$X$首先进入Multi-Head Attention层。\nMulti-Head Attention 对普通的Self-Attention，首先初始化三个可训练的、大小为$d_\\text{X} \\times d_{\\text{model}}$的权重矩阵$W^Q,W^K,W^V$。由矩阵乘积$X \\times W$分别得到$Q,K,V$三个矩阵，大小为$n\\times d_\\text{model}$。从而根据如下公式计算输出： $$ \\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_\\text{model}}})V $$ 最终输出尺寸为$ n\\times d_\\text{model}$。若不使用矩阵形式表示，普通Self-Attention相当于对每个token的$q$，计算与其他token的$k$向量的点积，softmax归一化之后作为该token的权重，最后将所有token的向量加权平均作为输出。\nMulti-Head Attention相当于使用数个不同的权重矩阵$W_i$，并行计算Self-Attention，最后将计算结果横向拼接。\n假设共有$8$个这样并行的Self-Attention层，若每一层Self-Attention的权重矩阵大小为$d_\\text{X} \\times d_\\text{v}$，则输出尺寸为$n\\times 8d_v$。论文中取$d_v=d_{model}/8$，从而在学习更多信息的前提下，使运算量与普通Self-Attention类似。\n对拼接后的结果$x_c$，乘上额外的参数矩阵$W^O$，最终得到输出尺寸为$ n\\times d_\\text{model}$，其中$W^O$尺寸为$8d_v\\times d_\\text{model}$。\n得到了该层的输出后，和残差网络类似的，论文使用了残差连接（residual connection）的方法连接两层，即将原始输入和该层输出直接相加，之后再对此应用Layer Normalization，传入下一层。公式表达如下： $$ \\text{LayerNorm}(x+\\text{Sublayer}(x)) $$\nPosition-wise Feed-Forward Networks Multi-Head Attention层后紧跟两个全连接层。其中第一个全连接层的神经元数量$d_{ff}=2048$，激活函数为ReLU，第二层的神经元数量为$d_\\text{model}$。公式表达如下： $$ \\text{FFN} = \\max(0,xW_1+b_1)W_2+b_2 $$ 对不同位置的token，全连接层的参数是共享的。 论文说这类似于两个卷积核尺寸为$1\\times1$​ 的卷积层，这样的卷积层和全连接层可以相互转换，并且根据卷积核维度随意调节输出维度。要注意的是，该前馈网络的对象是每个token的向量，而非整句话。","title":"Attention is All You Need阅读笔记"},{"content":"随机过程基本概念 最近主要学习了随机过程，故对书中一些概念进行整理，并附上自己的理解。一方面方便日后查阅，另一方面有助于在整理过程中查漏补缺。\n测度空间 首先对$\\sigma$代数的定义进行归纳。对于集合族F（集合组成的集合），其元素都是非空集合$\\Omega$的子集，若\n $\\Omega \\in F$（任何集合都是自身的子集）； 集合$A \\in F$，且补集$A^c \\in F$； 对于可数个$A_n \\in F$，其并集也属于F。  则$F$为$\\Omega$上的一个$\\sigma$代数，$(\\Omega,F)$称为可测空间。\n易知$F$中最大的元素就是$\\Omega$，联系概率的定义（$P(\\cdot)$是一个定义在$F$上的实值函数，且$P(\\Omega)=1,\\forall A\\in F\\text{有}0\\le P(A)\\le1$），可以发现概率的定义与$\\sigma$代数的定义一一对应，故$(\\Omega,F,P)$称为概率空间。\n随机过程 在概率空间上的一个随机变量集合 ${X(t),t\\in T}$ 就是随机过程，$T$被称为指标集或参数集。 若按照_性质 _进行划分，有两种随机过程非常重要，一个是 平稳过程，另一个是 独立增量过程。本文首先讨论平稳过程的定义和性质。\nNOTE: 此处对随机过程的分类并不是绝对的，一个具体的过程可以同时属于上述多种类型。以上思维导图只是为了帮助梳理本文结构。\n平稳过程 平稳过程又分为严平稳和宽平稳。严平稳要求随机变量的有限维分布不随时间变化 ；宽平稳要求随机过程的所有二阶矩存在 ，一阶矩不随时间变化且协方差函数只与时间间隔有关。\n一开始我顾名思义认为，严平稳要比宽平稳的要求更加严格，因此宽平稳应该包含严平稳。但查阅资料后发现，严平稳过程不一定存在二阶矩，因此不一定是宽平稳过程；而宽平稳过程的有限维分布也不一定不随时间变化，因此宽平稳过程也不一定满足严平稳的性质，具体的反例可见参考文献1。\n仔细分析严平稳和宽平稳的定义，可以发现如果一个严平稳的二阶矩存在，由于其分布不随时间变化，那么该严平稳过程的一阶矩必然不随时间变化，其协方差函数也与时间无关，满足宽平稳的所有要求。因此可以得出结论，存在二阶矩的严平稳过程必然是宽平稳的。\n对于平稳性而言，正态过程（有限维分布是多为正态分布）是一种很重要的特例。由于正态过程的有限维联合分布由均值函数和协方差函数唯一确定，若满足宽平稳条件（一阶矩不随时间变化、协方差函数只与时间间隔有关）则必然满足严平稳条件。故宽平稳的正态过程必然是严平稳的，所以在证明严平稳时，也可以从该性质入手。\n遍历性 接下来讨论平稳过程的性质——遍历性。首先给出均值遍历性的定义。\n对于平稳过程（或平稳序列）$X$，均值为$\\mu$。若满足\n$$ \\lim_{T \\to \\infty}\\frac{1}{2T}\\int_{-T}^TX(t)dt=\\mu $$\n或\n$$ \\lim_{T \\to \\infty}\\frac{1}{2N+1}\\sum_{k=-N}^NX(k)=\\mu $$\n则称$X$的均值具有遍历性。那么该如何理解均值的遍历性呢？\n注意均值函数的定义$\\mu_X(t) = E[X(t)]$，这是一个关于时间变化的函数。若要对某时刻的函数取值进行估计，通常我们要做的就是对处于时刻的随机过程进行大量观测，获得一系列类似${X_1(t),X_2(t),\\dots,X_n(t)}$的观测值，然后求他们的算术平均数。\n但是在现实中，这样同一时刻的大量的观测是很难得到的。例如将是否下雨记为随机变量，其关于时间的变化就是一个随机过程，但无论如何，某年某日是否下雨这个事件只会发生一次，我们永远也不可能得到一系列关于该日是否下雨的数据。那要如何估计下雨的概率呢？多观测几日后对数据取平均值不就好了！（此处不考虑季节等时间因素对降雨的影响），其实这在无意中就已经使用了均值的遍历性。\n对于协方差也有类似的遍历性定义，在此不再赘述。如果一个随机过程的均值和协方差函数都具有遍历性，则称此过程具有遍历性。\n独立增量过程 常识告诉我们，事物当前的状况和它过去的状况之间，往往有着千丝万缕的联系。同样的，大多数随机过程通常不满足独立性，但好在许多过程的增量是相互独立的，这为我们研究提供了极大便利。我们称满足这种条件的随机过程为独立增量过程。即对${X(t),t \\in T}$，增量$\\Delta_i = X(t_{i+1})-X(t_i)$相互独立。\n平稳独立增量过程 类似前文叙述的平稳过程定义，如果过程的增量的分布与时间无关，则称为平稳增量过程。同时具有独立增量和平稳增量的过程称为平稳独立增量过程。\n和证明严平稳一样，要证明增量平稳并不容易。好在可以通过特征函数入手，若${X(t),t \\ge 0}$是一个独立增量过程，其具有平稳增量的充分必要条件是：其特征函数具有可乘性，即\n$$ \\Psi_{X(t+s)}(a)=\\Psi_{X(t)}(a)\\Psi_{X(s)}(a) $$\n该定理可以通过特征函数的定义证明，此处略。\n参考文献  严平稳和宽平稳 平稳过程基本概念及例题 有限维分布的相容性  ","permalink":"https://exiarepairii.github.io/blog/post/%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","summary":"随机过程基本概念 最近主要学习了随机过程，故对书中一些概念进行整理，并附上自己的理解。一方面方便日后查阅，另一方面有助于在整理过程中查漏补缺。\n测度空间 首先对$\\sigma$代数的定义进行归纳。对于集合族F（集合组成的集合），其元素都是非空集合$\\Omega$的子集，若\n $\\Omega \\in F$（任何集合都是自身的子集）； 集合$A \\in F$，且补集$A^c \\in F$； 对于可数个$A_n \\in F$，其并集也属于F。  则$F$为$\\Omega$上的一个$\\sigma$代数，$(\\Omega,F)$称为可测空间。\n易知$F$中最大的元素就是$\\Omega$，联系概率的定义（$P(\\cdot)$是一个定义在$F$上的实值函数，且$P(\\Omega)=1,\\forall A\\in F\\text{有}0\\le P(A)\\le1$），可以发现概率的定义与$\\sigma$代数的定义一一对应，故$(\\Omega,F,P)$称为概率空间。\n随机过程 在概率空间上的一个随机变量集合 ${X(t),t\\in T}$ 就是随机过程，$T$被称为指标集或参数集。 若按照_性质 _进行划分，有两种随机过程非常重要，一个是 平稳过程，另一个是 独立增量过程。本文首先讨论平稳过程的定义和性质。\nNOTE: 此处对随机过程的分类并不是绝对的，一个具体的过程可以同时属于上述多种类型。以上思维导图只是为了帮助梳理本文结构。\n平稳过程 平稳过程又分为严平稳和宽平稳。严平稳要求随机变量的有限维分布不随时间变化 ；宽平稳要求随机过程的所有二阶矩存在 ，一阶矩不随时间变化且协方差函数只与时间间隔有关。\n一开始我顾名思义认为，严平稳要比宽平稳的要求更加严格，因此宽平稳应该包含严平稳。但查阅资料后发现，严平稳过程不一定存在二阶矩，因此不一定是宽平稳过程；而宽平稳过程的有限维分布也不一定不随时间变化，因此宽平稳过程也不一定满足严平稳的性质，具体的反例可见参考文献1。\n仔细分析严平稳和宽平稳的定义，可以发现如果一个严平稳的二阶矩存在，由于其分布不随时间变化，那么该严平稳过程的一阶矩必然不随时间变化，其协方差函数也与时间无关，满足宽平稳的所有要求。因此可以得出结论，存在二阶矩的严平稳过程必然是宽平稳的。\n对于平稳性而言，正态过程（有限维分布是多为正态分布）是一种很重要的特例。由于正态过程的有限维联合分布由均值函数和协方差函数唯一确定，若满足宽平稳条件（一阶矩不随时间变化、协方差函数只与时间间隔有关）则必然满足严平稳条件。故宽平稳的正态过程必然是严平稳的，所以在证明严平稳时，也可以从该性质入手。\n遍历性 接下来讨论平稳过程的性质——遍历性。首先给出均值遍历性的定义。\n对于平稳过程（或平稳序列）$X$，均值为$\\mu$。若满足\n$$ \\lim_{T \\to \\infty}\\frac{1}{2T}\\int_{-T}^TX(t)dt=\\mu $$\n或\n$$ \\lim_{T \\to \\infty}\\frac{1}{2N+1}\\sum_{k=-N}^NX(k)=\\mu $$\n则称$X$的均值具有遍历性。那么该如何理解均值的遍历性呢？\n注意均值函数的定义$\\mu_X(t) = E[X(t)]$，这是一个关于时间变化的函数。若要对某时刻的函数取值进行估计，通常我们要做的就是对处于时刻的随机过程进行大量观测，获得一系列类似${X_1(t),X_2(t),\\dots,X_n(t)}$的观测值，然后求他们的算术平均数。\n但是在现实中，这样同一时刻的大量的观测是很难得到的。例如将是否下雨记为随机变量，其关于时间的变化就是一个随机过程，但无论如何，某年某日是否下雨这个事件只会发生一次，我们永远也不可能得到一系列关于该日是否下雨的数据。那要如何估计下雨的概率呢？多观测几日后对数据取平均值不就好了！（此处不考虑季节等时间因素对降雨的影响），其实这在无意中就已经使用了均值的遍历性。\n对于协方差也有类似的遍历性定义，在此不再赘述。如果一个随机过程的均值和协方差函数都具有遍历性，则称此过程具有遍历性。\n独立增量过程 常识告诉我们，事物当前的状况和它过去的状况之间，往往有着千丝万缕的联系。同样的，大多数随机过程通常不满足独立性，但好在许多过程的增量是相互独立的，这为我们研究提供了极大便利。我们称满足这种条件的随机过程为独立增量过程。即对${X(t),t \\in T}$，增量$\\Delta_i = X(t_{i+1})-X(t_i)$相互独立。\n平稳独立增量过程 类似前文叙述的平稳过程定义，如果过程的增量的分布与时间无关，则称为平稳增量过程。同时具有独立增量和平稳增量的过程称为平稳独立增量过程。\n和证明严平稳一样，要证明增量平稳并不容易。好在可以通过特征函数入手，若${X(t),t \\ge 0}$是一个独立增量过程，其具有平稳增量的充分必要条件是：其特征函数具有可乘性，即","title":"随机过程基本概念"},{"content":"  蓝色和绿色两个分支代表两个头，同样的输入同时进入两个头。每个头的操作都一样，但是拥有独立的权重。下文以其中一个头的操作为例。\n  对输入按通道求均值，获得通道数维度的向量（Subsample）。\n  输入进入一个两层的MLP（ReLu激活），得到维度不变的稀疏向量（Saliency Generator），可以视为各通道的重要性得分或权重。\n  该稀疏向量的均值即为图中的L1 loss，原文希望通过对L1 loss的优化得到稀疏解。\n  按照提前设置的比例将重要性得分最低的几个通道的权重设为0（Gate）。\n  对每个通道乘以权重，此时有的通道数值被扩大，有的通道数值被缩小，有的通道数值被变为0。然后做普通卷积，得到该头的输出。\n  把各头的输出按照通道拼接，得到动态分组卷积的输出。\n  维度推导  输入$(N,C,H,W)$ 分$h$个头，就创建$h$个Saliency Generator  Saliency Generator  原论文直言这部分参考了SENet的思想，遵循SEBlock的设计规范。\n  按通道求均值得到$AVG_X = (N,C)$\n  全连接+ReLu，$ReLu((N,C)\\times(C,\\frac{C}{d})) = ReLu((N,\\frac{C}{d}))$，$d$为squeeze rate，是超参数，原文设为16。\n  全连接+ReLu，$Mask = ReLu((N,\\frac{C}{d})\\times (\\frac{C}{d},C)) = ReLu((N,C))$\n  Gating Strategy 若$\\text{inactive_channels}=0$​​，$Mask$​​即为各通道的权重。若$\\text{inactive_channels}\u0026gt;0$​​，则把权重最低的inactive_channels个通道的权重设为$0$​​。\n 对每个头，对输入$(N,C,H,W)$，按维度$C$乘以权重$Mask$，获得新的feature_map。 把所有头得到的feature_map按通道拼接，获得$(N,h*C,H,W)$并按通道打乱（为什么打乱？）。 分$h$组卷积，输出$(N,C_{\\text{out}},H,W)$  参数量计算 普通卷积 $$ C_{\\text{in}}C_{\\text{out}}HW $$\n分组卷积 $$ \\frac{C_{\\text{in}}}{\\text{group_num}}C_{\\text{out}}HW $$\n动态分组卷积 $$ C_{\\text{in}}C_{\\text{out}}HW + (C_{\\text{in}}\\frac{C_{\\text{in}}}{d}+\\frac{C_{\\text{in}}}{d}C_{\\text{in}})h $$\n其中$d$为SEBlock的squeeze rate，$h$为头的数量。\n可能存在的问题 参数量大 从卷积核的参数上看，动态分组卷积和普通卷积需要的参数量是一致的。但是由于每个头内部都需要一个SEBlock计算通道权重，所以总参数量还要略多于普通卷积。\n然而从运用L1 loss，Gating Strategy可以看出，论文作者是希望得到一个稀疏的通道权重，从而限制每个头得到的信息，与普通卷积区别开来。这就意味着那些被稀疏权重设为0的通道，所对应的卷积核参数都是无用的。如果删除这部分权重，动态分组卷积在卷积核部分的参数量能够减小$h$倍。\n需要学习的内容多 涉及到动态权重的网络结构一般都比较难学习，特别是样本量不足时。在高光谱图像上应用动态分组卷积会发现过拟合比较严重。如果按照原论文的裁剪策略，随着训练次数逐渐增大Gating（强制把某些通道权重设为0）的数量，伴随着Gating通道数的增加，loss和acc的波动会很大，和原文在 CIFAR-10上的训练效果不符。\n很自然地想到，是否由于模型效果依赖那些被裁剪的通道才导致了这样的波动？在通道数本就不多的情况下，这样的裁剪策略是否意味着每次增加裁剪数量都要模型重新学习？会不会导致最终的模型得不到充足的训练（只占总epoch的1/4）？\n所以不如抛弃原文的Gradually pruning策略，从训练开始就使用最终的裁剪比例（如图中的$\\xi$）。\n而此时，既然一开始就有这么多通道被屏蔽（权重设为0），那对其他通道的增强（权重大）或减弱（权重小）是否还有意义？原文参考的SENet采用了相似的通道增强或减弱方案（直接乘以权重），但是我认为这是因为SENet没有裁剪这个步骤，所以必须通过增强和减弱拉开重要通道和不重要通道的差距。而动态分组卷积既然已经对不重要通道做出了裁剪，是否可以进一步剔除对通道的增强和减弱，从而减少模型的学习负担？\n所以可以尝试只根据SEBlock计算的通道权重裁剪最不重要的通道，对其他通道不做处理。\nL1 loss 原文为了在Saliency Generator得到尽可能稀疏的解，使用了L1 loss。最后通过梯度下降法优化。\n更具体的：原文在SEBlock阶段使用了ReLu激活的全连接网络生成每个通道的权重，故权重均大于等于0，从而权重的均值可以看作L1 loss。原文对权重均值直接进行梯度下降，希望得到稀疏解。\n我对梯度下降法能否产生稀疏解持怀疑态度。我认为即使loss形式类似一范数，但采用梯度下降法优化只能减小输出，不能达到得到稀疏解的目的。\n如果为了得到稀疏解，将LARS算法应用在深度学习，可能具有较大挑战，一方面缺乏理想通道权重标签（y），另一方面可能需要解决并行计算的问题。\n那原论文为什么希望得到稀疏解呢？\n原论文中SEBlock输出的权重会和通道相乘，达到增强或减弱通道信号的目的，如果某些权重因为太小而被Gating策略强制设为0，可能导致一定的信息丢失。因为模型可能依赖这部分被裁剪的权重信息。所以最理想的情况就是SEBlock输出的权重是稀疏的，经过Gating的权重和模型本身希望得到的权重就是一致的了，没有信息丢失。\n那简单地思考，如果应用上文的改进，是否意味着SEBlock的输出是否是稀疏解都无所谓？因为SEBlock输出只用作通道筛选，而无论输出何种权重，最终筛选出的通道数量是恒定的；另一方面SEBlock输出也不会作为通道权重进行增强，不需要考虑信息丢失。\n可能的改进 综上，有以下几个可能的改进\n 抛弃Gradually pruning策略，使用固定的裁剪比例。 仅根据SEBlock输出筛选通道，不再使用该输出进行通道增强。 将筛选出有用的通道组合，形成新的feature maps，进行卷积，达到减少参数量的目的。  ","permalink":"https://exiarepairii.github.io/blog/post/%E5%8A%A8%E6%80%81%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/","summary":"蓝色和绿色两个分支代表两个头，同样的输入同时进入两个头。每个头的操作都一样，但是拥有独立的权重。下文以其中一个头的操作为例。\n  对输入按通道求均值，获得通道数维度的向量（Subsample）。\n  输入进入一个两层的MLP（ReLu激活），得到维度不变的稀疏向量（Saliency Generator），可以视为各通道的重要性得分或权重。\n  该稀疏向量的均值即为图中的L1 loss，原文希望通过对L1 loss的优化得到稀疏解。\n  按照提前设置的比例将重要性得分最低的几个通道的权重设为0（Gate）。\n  对每个通道乘以权重，此时有的通道数值被扩大，有的通道数值被缩小，有的通道数值被变为0。然后做普通卷积，得到该头的输出。\n  把各头的输出按照通道拼接，得到动态分组卷积的输出。\n  维度推导  输入$(N,C,H,W)$ 分$h$个头，就创建$h$个Saliency Generator  Saliency Generator  原论文直言这部分参考了SENet的思想，遵循SEBlock的设计规范。\n  按通道求均值得到$AVG_X = (N,C)$\n  全连接+ReLu，$ReLu((N,C)\\times(C,\\frac{C}{d})) = ReLu((N,\\frac{C}{d}))$，$d$为squeeze rate，是超参数，原文设为16。\n  全连接+ReLu，$Mask = ReLu((N,\\frac{C}{d})\\times (\\frac{C}{d},C)) = ReLu((N,C))$\n  Gating Strategy 若$\\text{inactive_channels}=0$​​，$Mask$​​即为各通道的权重。若$\\text{inactive_channels}\u0026gt;0$​​，则把权重最低的inactive_channels个通道的权重设为$0$​​。\n 对每个头，对输入$(N,C,H,W)$，按维度$C$乘以权重$Mask$，获得新的feature_map。 把所有头得到的feature_map按通道拼接，获得$(N,h*C,H,W)$并按通道打乱（为什么打乱？）。 分$h$组卷积，输出$(N,C_{\\text{out}},H,W)$  参数量计算 普通卷积 $$ C_{\\text{in}}C_{\\text{out}}HW $$","title":"动态分组卷积论文笔记"},{"content":"主成分分析意在降维，因子分析意在分析数据内在结构。二者目的不同，但采取了很多相同的步骤，因此如果只用结论就非常容易混淆。为了明确二者差异，首先给出步骤以及推导。\n步骤 假设有$m$条样本数据，每条样本有$n$维特征，得到样本矩阵\n$$ X^* = \\left( \\begin{matrix} x^*_{11} \u0026amp; x^*_{12} \u0026amp; \\dots \u0026amp; x^*_{1n} \\\\\nx^*_{21} \u0026amp; x^*_{22} \u0026amp; \\dots \u0026amp; x^*_{2n} \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nx^*_{m1} \u0026amp; x^*_{m2} \u0026amp; \\dots \u0026amp; x^*_{mn} \\\\\n\\end{matrix} \\right) = \\left( \\begin{matrix} x^*_{1} \u0026amp; x^*_{2} \u0026amp; \\dots \u0026amp; x^*_{n} \\\\ \\end{matrix} \\right) $$ 为了方便计算协方差，首先对样本进行中心化处理（用协方差做主成分分析也可以不中心化，证明差不多），得到$X$，有$E(x_i) = 0$。\n计算$C = \\frac{1}{m}X^TX$，则$C$是$X$的协方差矩阵，证明如下：\n$$ x_1^Tx_1 = \\sum_{i=1}^{m}x^2_{i1} = m\\left(E(x_1^2) - E^2(x_1)\\right)=mCov(x_1,x_1) $$\n$$ x_1^Tx_2 = \\sum_{i=1}^{m}x_{i1}x_{i2} = m\\left(E(x_1x_2) - E(x_1)E(x_2)\\right)=mCov(x_1,x_2) $$\n$$ X^TX = m\\left( \\begin{matrix} Cov(x_1,x_1) \u0026amp; Cov(x_1,x_2) \u0026amp; \\dots \u0026amp; Cov(x_1,x_n) \\\\\nCov(x_2,x_1) \u0026amp; Cov(x_2,x_2) \u0026amp; \\dots \u0026amp; Cov(x_2,x_n) \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nCov(x_n,x_1) \u0026amp; Cov(x_n,x_2) \u0026amp; \\dots \u0026amp; Cov(x_n,x_n) \\\\\n\\end{matrix} \\right) $$\n故$C$即为$X$的协方差矩阵，得证。\n根据协方差矩阵的性质，知$C$为实对称矩阵。实对称矩阵必然可以相似对角化，故存在正交矩阵$P$，使得\n$$ P^TCP = \\Lambda = \\left( \\begin{matrix} \\lambda_1 \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \\\\\n0 \u0026amp; \\lambda_2 \u0026amp; \\dots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\n0 \u0026amp; 0 \u0026amp; \\dots \u0026amp; \\lambda_n \\\\\n\\end{matrix} \\right) $$\n其中$P$由$C$经正交化后的特征向量组成，记为\n$$ P = \\left( \\begin{matrix} p_1 \u0026amp; p_2 \u0026amp; \\dots \u0026amp; p_n \\\\\n\\end{matrix} \\right) $$\n主成分分析和因子分析从这一步开始不同。\n主成分分析 主成分分析的目标是把样本维度从$n$降低到$k$（$k = 1,2,\\dots,n-1$）。\n至此，我们其实已经在不损失信息的前提下，将原样本$X$中盘根错节的特征相互分离，得到了新样本$XP$。接下来只需要保留$XP$中包含信息最多的$k$个特征，即可达到降维的目的。\n为什么要将$XP$作为新样本呢？\n$$ \\Lambda = P^TCP = P^T(\\frac{1}{m}X^TX)P = \\frac{1}{m}(XP)^TXP\\\\\n$$\n对比$X$的协方差矩阵$C = \\frac{1}{m}X^TX$，可以发现上式意味着$\\Lambda$是$XP$的协方差矩阵。\n如果将$XP$看作新样本，我们会发现$XP$的各个特征互不相关，说明特征之间不包含重复的信息。这是非常优秀的性质，如果以这样的数据作为样本，在建模时我们就不必考虑特征的相互影响。\n而另一方面，线性代数中向量的坐标是由向量空间的基确定的。\n$$ XE = XPP^T $$\n根据基的概念，可以把$X$看作样本在基$E$下的坐标，而$XP$则是样本在基$P^T$下的坐标。\n因此，将$XP$作为新样本不仅有利于剔除不重要的成分，还保留了原始样本中的所有信息。\n那要如何选择主成分呢？显然我们希望得到的主成分包含更多原始样本的信息（即经过PCA损耗的信息更少），因此应该根据主成分包含的信息量进行筛选。而方差反映了特征的变动幅度，变动大的特征自然包含的信息就更多，故保留$XP$中方差最大的$k$个特征即可得到$m$个样本关于$k$个主成分的取值。\n因子分析 假设存在数个不可观测的潜在变量，由这些潜在变量的变化影响了观测样本中各个特征的变化。因子分析希望将这些潜在变量从样本数据中分离，并找出样本特征与这些潜在变量之间的关系。用数学公式表示如下\n$$ X = FA + \\varepsilon $$\n其中$A$被称作因子载荷矩阵，$F$为公共因子，$\\varepsilon$为特殊因子带来的影响，类似于回归分析中的随机误差。\n不考虑特殊因子，式中仍有两个未知项，难以分解。假设$Cov(F) = E,Cov(F,\\varepsilon)=0, \\varepsilon_i \\sim N(0,\\sigma_i^2)$，则有\n$$ Cov(X) = AA^T + Cov(\\varepsilon) = AA^T + D $$\n经此运算，式中的未知项$F$即被消去。根据前文的证明，有\n$$ \\begin{align} Cov(X) \u0026amp;= C = P\\Lambda P^T \\\\\n\u0026amp;= \\sum_{i=1}^n\\lambda_ip_ip_i^T \\\\\n\u0026amp;= \\sum_{i=1}^k\\lambda_ip_ip_i^T + D \\\\\n\u0026amp;= AA^T + D \\end{align} $$\n其中\n$$ A = \\left( \\begin{matrix} \\sqrt{\\lambda_1}p_1 \u0026amp; \\sqrt{\\lambda_2}p_2 \u0026amp; \\dots \\sqrt{\\lambda_k}p_k \\end{matrix} \\right) $$\n此式即为因子载荷矩阵的解。得到因子载荷矩阵后，可以将因子载荷看作观测值，因子得分看作系数，通过最小二乘法求得因子得分$F$。\n因子分析的目的在于解释样本变量的内在结构。为了让因子更方便解释，可以利用类似前文PCA的原理，通过正交变换，尽可能扩大$F$各因子方差，即\n$$ X = FA + \\varepsilon = FRR^TA + \\varepsilon $$\n这就是因子旋转，具体的旋转方法有很多，由于和本文关系不大，不再赘述。\nSPSS操作 SPSS进行主成分分析和因子分析非常容易混淆。\n使用SPSS进行主成分分析时给出的成分矩阵实际上是因子载荷矩阵$A$，故应该对其中列向量除以相应的$\\sqrt{\\lambda}$才能得到主成分系数$P$。这就使得SPSS给出的主成分分析和因子分析（不加旋转的）的前半部分结果是完全相同的，让本就步骤相似的两个方法更加难以区分。\n比较异同 二者的目的不同。主成分分析的目标是降维，通过坐标变换，在尽可能多地保留数据信息的前提下，将变量线性组合以降低维度。因子分析的目标是分析，在假定存在无法观测的潜在因子的基础上，分解出因子载荷与主要因子，用来分析数据的内在结构。只看分析结果的话，主成分是对原有特征的聚合，因子是对原有特征的分解。\n二者有很多步骤相同。两种分析都对样本进行了中心化处理，都计算了协方差矩阵，都进行了相似对角化，一直到筛选特征这一步为止，二者所做的工作都是相同的。\n我认为步骤相同只是巧合。主成分分析进行这些步骤是为了找到正交矩阵$P$，从而将特征相互剥离，以保证在剔除特征时尽可能多的保留信息，且不影响其他未被剔除的特征。因子分析进行这些步骤则是为了消除$F$，从而求解因子载荷矩阵。\n","permalink":"https://exiarepairii.github.io/blog/post/%E6%B5%85%E6%9E%90%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E5%92%8C%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90/","summary":"主成分分析意在降维，因子分析意在分析数据内在结构。二者目的不同，但采取了很多相同的步骤，因此如果只用结论就非常容易混淆。为了明确二者差异，首先给出步骤以及推导。\n步骤 假设有$m$条样本数据，每条样本有$n$维特征，得到样本矩阵\n$$ X^* = \\left( \\begin{matrix} x^*_{11} \u0026amp; x^*_{12} \u0026amp; \\dots \u0026amp; x^*_{1n} \\\\\nx^*_{21} \u0026amp; x^*_{22} \u0026amp; \\dots \u0026amp; x^*_{2n} \\\\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\\nx^*_{m1} \u0026amp; x^*_{m2} \u0026amp; \\dots \u0026amp; x^*_{mn} \\\\\n\\end{matrix} \\right) = \\left( \\begin{matrix} x^*_{1} \u0026amp; x^*_{2} \u0026amp; \\dots \u0026amp; x^*_{n} \\\\ \\end{matrix} \\right) $$ 为了方便计算协方差，首先对样本进行中心化处理（用协方差做主成分分析也可以不中心化，证明差不多），得到$X$，有$E(x_i) = 0$。\n计算$C = \\frac{1}{m}X^TX$，则$C$是$X$的协方差矩阵，证明如下：\n$$ x_1^Tx_1 = \\sum_{i=1}^{m}x^2_{i1} = m\\left(E(x_1^2) - E^2(x_1)\\right)=mCov(x_1,x_1) $$","title":"浅析主成分分析和因子分析"}]